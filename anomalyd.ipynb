{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddf99c3-48ed-4fb6-81a9-d91088e77c3e",
   "metadata": {},
   "source": [
    "- This project serves to detect anomalies within images using Convolutional Autoencoders\n",
    "- To determine whether an image is normal or an anomaly, \n",
    "reconstruction error and kernel density estimation (based on the vectors in the latent space) \n",
    "are used\n",
    "- The bottleneck layer output\n",
    "from the autoencoder is considered to be the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc043c-4e77-409e-8244-1ed9ce7aecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Necessary Imports\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import layers, preprocessing, Sequential\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a787c2-45fd-4d59-bf5a-9b990c837dd7",
   "metadata": {},
   "source": [
    "Create a wrapper for TensorFlow's 'image_dataset_from_directory' for additional functionality and ease of use in managing images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34af5e-b849-4b72-bef5-53d641c07cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset:\n",
    "    def __init__(self, directory, image_size, batch_size, labels):\n",
    "        '''\n",
    "        Parameters:\n",
    "           - directory (str): The path to the directory containing the image dataset. The directory should be organized\n",
    "             with subdirectories, each representing a class label and containing the images for that class.\n",
    "           - image_size (tuple): The target size of the images after resizing, specified as (height, width).\n",
    "           - batch_size (int): The number of images to include in each batch during training.\n",
    "           - labels (str or list): Specifies whether to infer labels from the directory structure ('inferred') or \n",
    "             explicitly provide them. When set to 'inferred', subdirectory names are used as labels.\n",
    "        '''\n",
    "        self.dataset = tensorflow.keras.preprocessing.image_dataset_from_directory(\n",
    "            directory,\n",
    "            image_size=image_size,\n",
    "            batch_size=batch_size,\n",
    "            labels=labels\n",
    "        )\n",
    "        self.labels = labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def map(self, *args, **kwargs):\n",
    "        return self.dataset.map(*args, **kwargs)\n",
    "\n",
    "    def batch(self, *args, **kwargs):\n",
    "        return self.dataset.batch(*args, **kwargs)\n",
    "\n",
    "    def prefetch(self, *args, **kwargs):\n",
    "        return self.dataset.prefetch(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a06c1c-3a11-4058-aba7-ac7d87e5837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a size and batch_size for later use\n",
    "# The reasons for doing this are: 1) Neural Networks expect images of consistent dimensions and 2) The size of the batch determines the speed and stability of training. \n",
    "# A large batch size may smooth out differences between individual images, thereby mmaking the learning process too uniform. \n",
    "# This might cause the model to skim over important details. \n",
    "# A small batch size may mean the model updates its understanding based on just a few examples at a time. \n",
    "# This can make the learning process noisy, as small batches might not represent the overall data well\n",
    "SIZE = 8 # Resizing an image to be 8 x 8 will result in significant data loss. Keeping this number small simply for speed of testing (will change to 128 X 128 later)\n",
    "batch_size = 64 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe556ec-5f7c-4986-bd73-843f74919c10",
   "metadata": {},
   "source": [
    "- Define three generators for training, validation, and anomaly detection\n",
    "- Resize the images to a specific size and group into batches for processing\n",
    "\n",
    "\n",
    "*Additional Notes*:\n",
    "- Labels for the images are automatically inferred based on the subdirectory names within each directory\n",
    "- The train_generator is used for training a model, the validation_generator is used to evaluate the model's performance on unseen data, and the anomaly_generator is specifically set up to detect anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8d713-ddf0-4e87-894d-8e2e48322f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = preprocessing.image_dataset_from_directory(\n",
    "    r'C:\\Users\\awais\\Downloads\\archive (1)\\noncloud_train',    \n",
    "    image_size=(SIZE, SIZE),\n",
    "    batch_size=batch_size,\n",
    "    labels='inferred'\n",
    ")\n",
    "\n",
    "validation_generator = preprocessing.image_dataset_from_directory(\n",
    "    r'C:\\Users\\awais\\Downloads\\archive (1)\\noncloud_test',\n",
    "    image_size=(SIZE, SIZE),\n",
    "    batch_size=batch_size,\n",
    "    labels='inferred'\n",
    ")\n",
    "\n",
    "anomaly_generator = preprocessing.image_dataset_from_directory(\n",
    "    r'C:\\Users\\awais\\Downloads\\archive (1)\\cloud',\n",
    "    image_size=(SIZE, SIZE),\n",
    "    batch_size=batch_size,\n",
    "    labels='inferred'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee609bf0-5ae3-49cb-8ac5-3d70a3e08fc2",
   "metadata": {},
   "source": [
    "**Define a rescaling layer to normalize image pixel values in a range of [0, 1]. Normalizing values ensures the gradient (partial derivative of the loss function) is not excessively large, leading to more stable and faster convergence (optimal state)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7a921-cd77-4114-84e7-7690eb6f6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaling_layer = layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347a3c7-653d-492d-81e3-4015102ae29f",
   "metadata": {},
   "source": [
    "*General Idea*:\n",
    " - 'change_inputs' function processes a batch of input images for use in an autoencoder by resizing and rescaling them to a specified size\n",
    "\n",
    "*Intricacies*:\n",
    "- Prints the shape of the original images, applies a rescaling transformation using rescaling_layer, and then resizes the images to [SIZE, SIZE] using nearest-neighbor interpolation\n",
    "- The function returns a tuple (x, x) where both elements are the resized and rescaled images. In the context of an autoencoder, this means the same processed images are used as both the input and target output, aligning with the autoencoder's goal of reconstructing its input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce723c-0e83-4222-ba8d-4d6f4352617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_inputs(images, labels):\n",
    "    '''\n",
    "    Parameters:\n",
    "        - images: A batch of input images.\n",
    "        - labels: Corresponding labels (currently not used in this function).\n",
    "\n",
    "    Returns:\n",
    "        - A tuple (x, x) where both elements are the processed images:\n",
    "        - x: The resized and rescaled images, which serve as both the input and the target output for the autoencoder.\n",
    "    '''\n",
    "    print(f\"Original images shape: {images.shape}\")\n",
    "    x = tensorflow.image.resize(rescaling_layer(images),[SIZE, SIZE], method=tensorflow.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    print(f\"Resized images shape: {x.shape}\")\n",
    "    return x, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a218c-69f0-4eac-bc04-0ec8627afebd",
   "metadata": {},
   "source": [
    "**Use map to apply the change_inputs function to each batch of images and labels in the datasets. This function resizes and rescales the images, so now each dataset—train_dataset, validation_dataset, and anomaly_dataset—has images that are prepared and ready for their respective tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9fa2a-4305-4753-81dd-31db948fdfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_generator.map(change_inputs)\n",
    "validation_dataset = validation_generator.map(change_inputs)\n",
    "anomaly_dataset = anomaly_generator.map(change_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d7541-4bec-46b5-aa8d-4f805872c73b",
   "metadata": {},
   "source": [
    "check_none_in_dataset checks if any images or labels in a dataset are None. It loops through each batch and prints a message if it finds any None values, returning True if it does. If everything looks good and there are no None values, it prints a confirmation message and returns False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1d360-955b-4ae7-8f5a-d45bd9bed67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_none_in_dataset(dataset):\n",
    "    '''\n",
    "    Parameters:\n",
    "        dataset (tf.data.Dataset): A TensorFlow dataset object containing batches of images and labels.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if any `None` values are found in the dataset; False otherwise. Prints a message indicating whether `None` values were found.\n",
    "    '''\n",
    "    for batch in dataset:\n",
    "        images, labels = batch\n",
    "        if images is None or labels is None:\n",
    "            print(\"Found None in dataset\")\n",
    "            return True\n",
    "    print(\"No None values in dataset\")\n",
    "    return False\n",
    "\n",
    "# Check validation dataset\n",
    "print(\"Checking validation dataset for None values:\")\n",
    "c = check_none_in_dataset(validation_dataset)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66e769-b0dd-4246-a92b-07a926902790",
   "metadata": {},
   "source": [
    "print_labels_from_dataset iterates through a specified number of batches from a given dataset, printing the labels associated with the images in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c955843-d5f7-4245-9238-f0d10d852c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_labels_from_dataset(dataset, num_batches=1):\n",
    "    '''\n",
    "    Parameters:\n",
    "        dataset (tf.data.Dataset): A TensorFlow dataset object containing batches of images and labels.\n",
    "        num_batches (int, optional): The number of batches to process from the dataset. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints the labels and a comparison between labels and images but does not return any values.\n",
    "    '''\n",
    "    for images, labels in dataset.take(num_batches):\n",
    "        print(\"Labels (should be the same as images):\")\n",
    "        print(labels.numpy())  # Print the labels to check if they are the expected values (not None)s\n",
    "        print(labels.numpy() == images.numpy())\n",
    "\n",
    "print(\"Validation Dataset Labels:\")\n",
    "bat = print_labels_from_dataset(validation_dataset)\n",
    "\n",
    "print(\"Anomaly Dataset Labels:\")\n",
    "cow = print_labels_from_dataset(anomaly_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce35cc-6f7d-4e7a-a2b6-129999fd91f2",
   "metadata": {},
   "source": [
    "- Define a sequential model \n",
    "- Deconstruct the images by using convolutional layers and max pooling to progressively downsample the input images, capturing essential features while reducing spatial dimensions\n",
    "- Reconstruct the images from the encoded features by using convolutional layers and upsampling layers to reverse the downsampling process\n",
    "- Use a sigmoid activation to output images with the same shape as the input\n",
    "\n",
    "*Extra Notes*:\n",
    "Try to minimize the dimensions of the latent space/bottleneck layer\n",
    "to retain as much information as possible.\n",
    "The size of the latent space should be small enough to force the model \n",
    "to learn an efficient compression but large enough to preserve the essential features of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed17fb-ef3a-4226-8dc6-81e2ad2636ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Encoder\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2), padding='same')) # reduce the spatial dimensions of the feature maps produced by layers.Conv2D by taking max value (this highlights the most important features) of every 2 x 2 window\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "# Decoder\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.UpSampling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.UpSampling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.UpSampling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6caa77-210c-46a0-93de-52c8ec349d60",
   "metadata": {},
   "source": [
    "print_shapes_and_check function examines and compares the shapes of images, labels, and predictions from a dataset using a specified model\n",
    "\n",
    "*Additional Information*:\n",
    "- Processes one batch of data, prints the shapes of the images and labels, makes predictions with the model, and prints the shape of those predictions\n",
    "\n",
    "- Checks if the shape of the predictions matches the shape of the labels, printing a boolean result to indicate whether they are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f864d9-5f08-4251-bc12-28ac005671a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shapes_and_check(dataset, model):\n",
    "    '''\n",
    "    Parameters:\n",
    "        dataset (tf.data.Dataset): A TensorFlow dataset object containing batches of images and labels.\n",
    "        model (tf.keras.Model): A Keras model used for making predictions.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints the shapes of images, labels, and predictions, and checks for shape mismatches, but does not return any values.\n",
    "    '''\n",
    "    for images, labels in dataset.take(1):  # Take one batch for checking\n",
    "        print(f\"Images shape: {images.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model(images, training=False)\n",
    "\n",
    "        # Print prediction and label shapes\n",
    "        print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "        # Check if shapes match\n",
    "        label_shape_correct = labels.shape == predictions.shape\n",
    "        print(f\"Labels shape matches predictions shape: {label_shape_correct}\")\n",
    "\n",
    "        # If shapes are not matching, print additional debugging info\n",
    "        if not label_shape_correct:\n",
    "            print(f\"Shape mismatch: Labels shape {labels.shape} vs. Predictions shape {predictions.shape}\")\n",
    "\n",
    "# Check training dataset\n",
    "print(\"Checking training dataset shapes:\")\n",
    "print_shapes_and_check(train_dataset, model)\n",
    "\n",
    "# Check validation dataset\n",
    "print(\"Checking validation dataset shapes:\")\n",
    "print_shapes_and_check(validation_dataset, model)\n",
    "\n",
    "# Check anomaly dataset\n",
    "print(\"Checking anomaly dataset shapes:\")\n",
    "print_shapes_and_check(anomaly_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba5d2b-1277-4102-b2ac-7df3cbb422c7",
   "metadata": {},
   "source": [
    "- evaluate_and_print_shapes function is designed to assess and report on the shapes and performance of a model on a given dataset\n",
    "\n",
    "*Additional Information*:\n",
    "- Processes one batch from the dataset, printing the shapes of the images and labels\n",
    "- Checks if the shape of the predictions matches the shape of the labels\n",
    "- Calculates the Mean Squared Error (MSE) between the labels and predictions, printing both the shape and value of the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a3735-5115-474a-9d41-a7619b279508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_print_shapes(dataset, model):\n",
    "    '''\n",
    "    Paramters:\n",
    "        dataset (tf.data.Dataset): A TensorFlow dataset object containing batches of images and labels.\n",
    "        model (tf.keras.Model): A Keras model used for making predictions.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints the shapes of images, labels, and predictions, checks for shape mismatches, and computes the MSE, but does not return any values.\n",
    "    '''\n",
    "    for batch in dataset.take(1):  # Take one batch for checking\n",
    "        images, labels = batch\n",
    "        print(f\"Batch images shape: {images.shape}\")\n",
    "        print(f\"Batch labels shape: {labels.shape}\")\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model(images, training=False)\n",
    "\n",
    "        # Print prediction and label shapes\n",
    "        print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "        # Check if shapes match\n",
    "        label_shape_correct = labels.shape == predictions.shape\n",
    "        print(f\"Labels shape matches predictions shape: {label_shape_correct}\")\n",
    "\n",
    "        # If shapes are not matching, print additional debugging info\n",
    "        if not label_shape_correct:\n",
    "            print(f\"Shape mismatch: Labels shape {labels.shape} vs. Predictions shape {predictions.shape}\")\n",
    "\n",
    "        # Compute and print the Mean Squared Error (MSE)\n",
    "        mse = tensorflow.keras.losses.MeanSquaredError()\n",
    "        error = mse(labels, predictions)\n",
    "        print(f\"Mean Squared Error shape: {error.shape}\")\n",
    "        print(f\"Mean Squared Error value: {error.numpy()}\")\n",
    "\n",
    "# Evaluate and print shapes for validation dataset\n",
    "print(\"Evaluating validation dataset:\")\n",
    "evaluate_and_print_shapes(validation_dataset, model)\n",
    "\n",
    "# Evaluate and print shapes for anomaly dataset\n",
    "print(\"Evaluating anomaly dataset:\")\n",
    "evaluate_and_print_shapes(anomaly_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4241f1-49c3-49e9-b733-8efd1d5eea1a",
   "metadata": {},
   "source": [
    "- Train the model for 1000 epochs using train_dataset\n",
    "- The shuffle=True option ensures that the training data is shuffled before each epoch to improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d58b7-4869-460f-9070-7fbbd42fa919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch = 1500 // batch_size,\n",
    "    epochs = 1000,\n",
    "    validation_data = validation_dataset,\n",
    "    validation_steps = 225 // batch_size,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b09bdd-126f-4008-bf65-562eefbeaa3e",
   "metadata": {},
   "source": [
    "Plots the training and validation loss over epochs, using yellow for training loss and red for validation loss, with appropriate labels and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60e069-7914-440f-bf03-e70747a3e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6159d-51f0-4c56-a709-ec4251e0b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all batches generated by the datagen and pick a batch for prediction\n",
    "# Just to test the model.\n",
    "data_batch = []  # Capture all training batches as a numpy array\n",
    "\n",
    "# Iterate over the dataset\n",
    "for images, _ in train_dataset:\n",
    "    data_batch.append(images.numpy())\n",
    "\n",
    "# Convert the list of numpy arrays to a single numpy array\n",
    "data_batch = np.concatenate(data_batch, axis=0)\n",
    "\n",
    "# Predict using the model\n",
    "predicted = model.predict(data_batch)\n",
    "\n",
    "# View a couple images and recon.\n",
    "\n",
    "image_num = random.randint(0, data_batch[0].shape[0] - 1) \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data_batch[0][image_num])\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted[image_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26cdc4-3ae8-4097-a126-9bf7ac1673d6",
   "metadata": {},
   "source": [
    "- Print the shape of a batch of images from both the anomaly and validation generators, stopping after the first batch from each generator\n",
    "- Examine the reconstruction error in the validation data and anomaly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec946898-84fc-418b-8c5c-8a9cd54347f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, _ in anomaly_generator:\n",
    "    print(f\"Anomaly batch shape: {images.shape}\")\n",
    "    break\n",
    "\n",
    "for images, _ in validation_generator:\n",
    "    print(f\"Validation batch shape: {images.shape}\")\n",
    "    break\n",
    "\n",
    "\n",
    "anomaly_error = model.evaluate(anomaly_dataset)\n",
    "validation_error = model.evaluate(validation_dataset)\n",
    "\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Recon. error for the validation data is {validation_error}\")\n",
    "print(f\"Recon. error for the anomaly data is {anomaly_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1302b0-3d07-46f1-9895-8706cab986a5",
   "metadata": {},
   "source": [
    "- Construct an encoder model by sequentially adding convolutional and max pooling layers, replicating the structure of the encoder portion of the previous model\n",
    "\n",
    "*Additional Information*:\n",
    "Each convolutional layer in the encoder is initialized with weights taken from corresponding layers in the other model, ensuring that the encoder starts with the same learned features as the original model. This encoder processes input images to extract their latent space representations, which can then be used for further analysis, such as calculating Kernel Density Estimates (KDE) to understand the distribution of these representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d095e-c5af-4be8-bf46-7be069fc7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Sequential()\n",
    "encoder.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', use_bias=True, input_shape=(SIZE, SIZE, 3)))\n",
    "x = model.layers[0].get_weights()\n",
    "encoder.layers[0].set_weights(x)\n",
    "encoder.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "encoder.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', use_bias=True))\n",
    "y = model.layers[2].get_weights()\n",
    "encoder.layers[2].set_weights(y)\n",
    "encoder.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "encoder.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same', use_bias=True))\n",
    "z = model.layers[4].get_weights()\n",
    "encoder.layers[4].set_weights(z)\n",
    "encoder.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4488874-5883-4286-b1e3-fc15d0f0d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get encoded output of input images = Latent space\n",
    "encoded_images = encoder.predict(train_dataset)\n",
    "\n",
    "# Flatten the encoder output because KDE takes 1D vectors as input\n",
    "encoder_output_shape = encoder.output_shape \n",
    "out_vector_shape = encoder_output_shape[1] * encoder_output_shape[2] * encoder_output_shape[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20930c8a-f203-48d7-af64-71af08b8c539",
   "metadata": {},
   "source": [
    "- Reshape each image in encoded_images to a specified vector shape and append the reshaped images to a list\n",
    "- Convert this list into a NumPy array and fit a Kernel Density Estimate (KDE) model with a Gaussian kernel and a bandwidth of 0.2 to the reshaped image vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f34dd-dcad-46e2-8bc3-8a5f2a85402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images_vector = []\n",
    "\n",
    "# Reshape each image and append to the list\n",
    "for img in encoded_images:\n",
    "    reshaped_img = np.reshape(img, out_vector_shape)\n",
    "    encoded_images_vector.append(reshaped_img)\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "encoded_images_vector = np.array(encoded_images_vector)\n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(encoded_images_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae737dff-af59-433f-ad11-b537a7751ee6",
   "metadata": {},
   "source": [
    "- Compute the density and reconstruction error for a batch of images, using the new encoder and KDE model\n",
    "\n",
    "*Additional Information*:\n",
    "It reshapes each image, predicts its encoded form, calculates its density using KDE, and evaluates the reconstruction error by comparing the reconstructed image to the original. The function returns the average and standard deviation of both density and reconstruction error. The function is then applied to batches of images from train_generator and anomaly_generator to obtain these statistics for both uninfected and anomaly images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1371e9-77ff-4d56-9d8b-041acf8e96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_density_and_recon_error(batch_images):\n",
    "    '''\n",
    "    Parameters:\n",
    "        batch_images (np.ndarray or tensor): A batch of images, where each image is expected to be preprocessed to match the input shape required by the encoder and model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing four values:\n",
    "            - average_density (float): The mean density score of the images in the batch.\n",
    "            - stdev_density (float): The standard deviation of the density scores.\n",
    "            - average_recon_error (float): The mean reconstruction error of the images in the batch.\n",
    "            - stdev_recon_error (float): The standard deviation of the reconstruction errors.\n",
    "    '''\n",
    "    density_list=[]\n",
    "    recon_error_list=[]\n",
    "    for im in range(0, batch_images.shape[0]-1):\n",
    "        \n",
    "        img = batch_images[im][np.newaxis, :,:,:]\n",
    "        encoded_img = encoder.predict([[img]]) \n",
    "        for i in range(len(encoded_img)):\n",
    "            encoded_img[i] = np.reshape(encoded_img[i], out_vector_shape)\n",
    "        density = kde.score_samples(encoded_img)[0] \n",
    "        reconstruction_error = model.evaluate([model.predict([[img]])],[[img]], batch_size = 1)[0]\n",
    "        density_list.append(density)\n",
    "        recon_error_list.append(reconstruction_error)\n",
    "        \n",
    "    average_density, stdev_density = np.mean(np.array(density_list)), np.std(np.array(density_list)) \n",
    "   \n",
    "    average_recon_error, stdev_recon_error = np.mean(np.array(recon_error_list)), np.std(np.array(recon_error_list)) \n",
    "\n",
    "    \n",
    "    return average_density, stdev_density, average_recon_error, stdev_recon_error\n",
    "\n",
    "\n",
    "# Get average and std dev. of density and recon. error for uninfected and anomaly images. \n",
    "train_batch = next(iter(train_generator))[0]\n",
    "anomaly_batch = next(iter(anomaly_generator))[0]\n",
    "\n",
    "uninfected_values = calc_density_and_recon_error(train_batch)\n",
    "anomaly_values = calc_density_and_recon_error(anomaly_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e8083-b6b7-4867-b6ef-bbe1fa6ddc29",
   "metadata": {},
   "source": [
    "- Evaluate whether a given image is an anomaly based on density and reconstruction error thresholds\n",
    "\n",
    "*Additional Information*:\n",
    "It loads and preprocesses the image by resizing it to SIZExSIZE pixels and ensuring it has three color channels. The image is then normalized and reshaped for model input. The function uses the encoder to obtain the encoded representation and calculates its density with the KDE model. It also reconstructs the image using the model and computes the reconstruction error. If the density is below a specified threshold or the reconstruction error exceeds another threshold, the function classifies the image as an anomaly; otherwise, it considers it as not an anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2214682-2db4-451f-971e-989b455626e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anomaly(img_path):\n",
    "    '''\n",
    "    Parameters:\n",
    "        img_path (str): The file path to the image to be checked.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints whether the image is an anomaly or not based on predefined thresholds.\n",
    "    \n",
    "    '''\n",
    "    density_threshold = 11 # Set this value based on the above exercise\n",
    "    reconstruction_error_threshold = 0.00014 # Set this value based on the above exercise\n",
    "    img  = Image.open(img_path)\n",
    "    img = np.array(img.resize((SIZE,SIZE), Image.Resampling.LANCZOS))\n",
    "    if img.shape[-1] != 3:\n",
    "        img = np.stack((img,) * 3, axis=-1) \n",
    "    \n",
    "    plt.imshow(img)\n",
    "    img = img / 255.\n",
    "    img = img[np.newaxis, :,:,:]\n",
    "    encoded_img = encoder.predict(img) \n",
    "    encoded_img = [np.reshape(img, (out_vector_shape)) for img in encoded_img] \n",
    "    density = kde.score_samples(encoded_img)[0] \n",
    "\n",
    "    reconstruction = model.predict([[img]])\n",
    "    reconstruction_error = model.evaluate([reconstruction],[[img]], batch_size = 1)[0]\n",
    "\n",
    "    if density < density_threshold or reconstruction_error > reconstruction_error_threshold:\n",
    "        print(\"The image is an anomaly\")\n",
    "        \n",
    "    else:\n",
    "        print(\"The image is not an anomaly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
